<p align="center">
  <h1 align="center">ğŸ¤Ÿ WhisperSign</h1>
  <p align="center">
    <strong>Modified OpenAI Whisper for Real-Time Sign Language Recognition</strong><br>
    <em>From Audio Spectrograms to 3D Skeletal Data</em>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch&logoColor=white" alt="PyTorch">
    <img src="https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white" alt="Python">
    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="MIT License">
    <img src="https://img.shields.io/badge/Tests-112%20passed-brightgreen" alt="Tests">
  </p>
</p>

---

## ğŸ“– Overview

**WhisperSign** reimagines OpenAI's Whisper architecture for sign language recognition. Instead of processing audio via Log-Mel Spectrograms, it accepts **3D skeletal hand data** `(T Ã— 42 Ã— 7)` from Leap Motion or MediaPipe â€” 42 hand joints (21 per hand) with 7 features each (x, y, z, velocity_x, velocity_y, velocity_z, confidence).

The model outputs **sign glosses** â€” semantic labels for individual signs â€” enabling real-time translation of hand gestures into text.

### Why Modify Whisper?

| Challenge | Whisper's Strength | Our Adaptation |
|-----------|-------------------|----------------|
| Temporal sequence modeling | Proven on variable-length audio | Applied to variable-length gesture sequences |
| Noisy real-world input | Robust to audio noise | Robust to skeletal tracking noise |
| Multi-scale pattern detection | Phoneme â†’ word â†’ sentence | Finger config â†’ hand shape â†’ sign phrase |
| Real-time streaming | Efficient attention mechanism | Sliding window inference for live translation |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WhisperSign Pipeline                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Leap Motion / MediaPipe                                    â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  Raw Skeletal Data (T, 42, 7)                               â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  FRONTEND (replaces Mel Spectrogram)                     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚  â”‚ Temporal Patch Embeddingâ”‚  Groups P frames â†’ patches  â”‚
â”‚  â”‚  â”‚ (T,42,7) â†’ (T/P, d)    â”‚  Reduces sequence T â†’ T/P  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚  â”‚           â–¼                                              â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚  â”‚ ConvSPE                 â”‚  Learns spatial-positional  â”‚
â”‚  â”‚  â”‚ Depthwise + Pointwise   â”‚  relationships dynamically  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚  â”‚           â–¼                                              â”‚
â”‚  â”‚  BatchNorm â†’ SpatialDropout1D                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  ENCODER (Spatio-Temporal Blocks Ã— N)                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚  â”‚ S-MHSA                  â”‚  Spatial: handshape at t    â”‚
â”‚  â”‚  â”‚ (Spatial Self-Attention)â”‚  "What fingers are where?"  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚  â”‚           â–¼                                              â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚  â”‚ T-MHSA + RPE            â”‚  Temporal: motion over timeâ”‚
â”‚  â”‚  â”‚ (Temporal Self-Attention)â”‚  "How does the hand move?" â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚  â”‚           â–¼                                              â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚  â”‚ Feed-Forward (Pre-Norm) â”‚  GELU activation            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  DECODER (Two-Pass)                                      â”‚
â”‚  â”‚                                                          â”‚
â”‚  â”‚  Pass 1: CTC Head â”€â”€â”€â”€â”€â”€â–º Fast monotonic alignment       â”‚
â”‚  â”‚          (Linear â†’ LogSoftmax â†’ Greedy Decode)           â”‚
â”‚  â”‚                                                          â”‚
â”‚  â”‚  Pass 2: Attention Decoder â”€â–º Rescoring for accuracy     â”‚
â”‚  â”‚          (Transformer Decoder with causal mask)          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚              â–¼                                              â”‚
â”‚       Sign Glosses: ["hello", "thank_you", "please"]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Architecture Decisions

| Component | Design Choice | Rationale |
|-----------|--------------|-----------|
| **Frontend** | Temporal Patch Embedding (not Conv2D) | Skeletal data is structured (joints Ã— features), not pixels. Patches group P consecutive frames. |
| **Positional Encoding** | ConvSPE (learned, convolutional) | Hand skeleton has fixed topology â€” learned spatial relationships outperform sinusoidal PE. |
| **Encoder Attention** | Dual S-MHSA + T-MHSA (not unified) | Separating spatial and temporal attention allows the model to independently reason about *what the hand looks like* vs *how it moves*. |
| **Temporal Attention** | Relative Positional Encoding (RPE) | Sign dynamics depend on *relative* timing (how long between movements), not absolute position. |
| **Decoder** | CTC + Attention (two-pass) | CTC provides fast, monotonic alignment; Attention rescoring improves accuracy. Combined hybrid loss: `L = Î±Â·CTC + (1-Î±)Â·Attention`. |
| **Normalization** | Pre-Norm (LayerNorm before attention) | More stable training with deeper networks, better gradient flow. |

---

## âœ¨ Features

### Data Pipeline
- **Spline Interpolation Resampling** â€” Converts variable frame rates to fixed 60 Hz using cubic spline interpolation
- **Hand-Centric Spatial Normalization** â€” Translates coordinates so palm joint is at origin (left and right hands independently)
- **Scale Normalization** â€” Normalizes by metacarpal bone length for hand-size invariance
- **Feature Scaling** â€” StandardScaler or MinMaxScaler applied across the dataset
- **Gesture Masking Augmentation** â€” Randomly masks joints or temporal segments (like SpecAugment for audio)
- **Temporal Jitter** â€” Random frame shifting for temporal robustness
- **Noise Injection** â€” Gaussian noise to simulate sensor inaccuracy

### Model
- **4.2M parameters** (base config, d_model=512) â€” lightweight enough for real-time inference
- **Configurable depth** â€” Scale from tiny (d_model=128) to large (d_model=768)
- **Freeze/Unfreeze API** â€” Selective component training for transfer learning
- **Checkpoint save/load** â€” Full state persistence including epoch, loss, and config

### Training
- **3-Stage Progressive Training**
  - Stage 1: Frontend warm-up (encoder + decoder frozen)
  - Stage 2: Joint training with hybrid CTC-Attention loss
  - Stage 3: Real-time optimization with sliding window augmentation
- **Hybrid CTC-Attention Loss** with configurable weight Î±
- **Cosine Warmup Scheduler** â€” Linear warmup followed by cosine annealing
- **Gradient Clipping** â€” Prevents training instability
- **TensorBoard Logging** â€” Real-time loss and metric visualization

### Inference
- **Sliding Window Inference** â€” Process continuous streams in real-time with configurable overlap
- **Moving Average Smoothing** â€” Reduces sensor noise in live data
- **MediaPipe Integration** â€” Extract hand keypoints directly from video

---

## ğŸ“Š Technical Specifications

### Model Configurations

| Config | d_model | Layers | Heads | Params | GPU Memory | Use Case |
|--------|---------|--------|-------|--------|------------|----------|
| Tiny | 128 | 2 | 4 | ~1.1M | ~2 GB | Prototyping, edge devices |
| Base | 256 | 4 | 4 | ~4.2M | ~4 GB | Balanced performance |
| **Default** | **512** | **6** | **8** | **~18M** | **~8 GB** | **Recommended** |
| Large | 768 | 8 | 12 | ~45M | ~16 GB | Maximum accuracy |

### Input Format

| Feature | Index | Description |
|---------|-------|-------------|
| x, y, z | 0-2 | 3D joint coordinates (meters) |
| vx, vy, vz | 3-5 | Joint velocities (m/s) |
| confidence | 6 | Tracking confidence [0, 1] |

**Joint Layout:** 21 joints per hand Ã— 2 hands = 42 joints total (following MediaPipe hand landmark convention)

### Training Pipeline

```
Stage 1: Frontend Warm-up          Stage 2: Joint Training          Stage 3: Real-time Opt.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend: TRAINABLE  â”‚     â”‚ Frontend: TRAINABLE      â”‚     â”‚ Frontend: TRAINABLE    â”‚
â”‚ Encoder:  FROZEN     â”‚ â”€â”€â–º â”‚ Encoder:  TRAINABLE      â”‚ â”€â”€â–º â”‚ Encoder:  TRAINABLE    â”‚
â”‚ Decoder:  FROZEN     â”‚     â”‚ Decoder:  TRAINABLE      â”‚     â”‚ Decoder:  TRAINABLE    â”‚
â”‚ LR: 1e-3             â”‚     â”‚ LR: 5e-5                 â”‚     â”‚ LR: 1e-5              â”‚
â”‚ Loss: CTC only       â”‚     â”‚ Loss: 0.3Â·CTC + 0.7Â·ATT â”‚     â”‚ Loss: 0.3Â·CTC+0.7Â·ATT â”‚
â”‚ Epochs: 30           â”‚     â”‚ Epochs: 100              â”‚     â”‚ Epochs: 30             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª Verification

The model has been verified with **112 tests** across two test suites:

### Structural Tests (68/68 passed)
- Tensor shape propagation through all components
- Gradient flow from loss to every trainable parameter
- Hybrid loss formula correctness (`L = Î±Â·CTC + (1-Î±)Â·Attention`)
- Checkpoint save/load round-trip (weights identical)
- Numerical stability with extreme inputs (Ã—100, Ã—0.001)
- Edge cases: batch=1, minimum sequence length, all-zero input

### Functional Tests (44/44 passed)
- **CTC Decoding**: Blank removal, deduplication, alternating patterns all correct
- **Causal Mask**: Verified no future information leakage in attention decoder
- **Encoder Masking**: Padded positions properly ignored (cosine similarity = 0.975)
- **RPE**: Shift-invariant, distance-differentiating relative position encoding
- **Memorization**: Loss 3.678 â†’ 0.006 in 80 steps, 4/4 samples decoded correctly
- **Gradient Health**: No vanishing/exploding, frontendâ†”encoder ratio = 1.3Ã—
- **End-to-End**: Full numpy â†’ normalize â†’ preprocess â†’ model â†’ decode pipeline

Run the tests yourself:
```bash
python scripts/smoke_test.py       # Quick sanity check (~10s)
python scripts/deep_test.py        # Structural tests (~30s)
python scripts/functional_test.py  # Functional tests (~60s)
```

---

## ğŸš€ Getting Started

### Installation

```bash
git clone https://github.com/YOUR_USERNAME/Whisper_modification.git
cd Whisper_modification
pip install -r requirements.txt
```

### Quick Verification

```bash
python scripts/smoke_test.py
```

### Training

```bash
# Train all 3 stages sequentially
python scripts/train.py --config configs/config.yaml --data_dir data/processed

# Train individual stages
python scripts/train.py --config configs/config.yaml --stage 1
python scripts/train.py --config configs/config.yaml --stage 2 --resume checkpoints/best_stage1.pt
python scripts/train.py --config configs/config.yaml --stage 3 --resume checkpoints/best_stage2.pt

# Specify device
python scripts/train.py --config configs/config.yaml --device cuda
```

### Inference

```python
import torch
from src.model.whisper_sign import WhisperSignModel

# Load trained model
model, _ = WhisperSignModel.load_checkpoint("checkpoints/final_model.pt")
model.eval()

# Run inference on skeletal data
data = torch.randn(1, 120, 42, 7)  # (batch, frames, joints, features)
lengths = torch.tensor([120])
predictions = model.decode(data, lengths)
print(f"Predicted signs: {predictions[0]}")
```

### Real-Time Streaming

```python
from src.utils.sliding_window import SlidingWindowInference
from src.utils.smoothing import MovingAverageSmoothing

# Setup
model, _ = WhisperSignModel.load_checkpoint("checkpoints/final_model.pt")
smoother = MovingAverageSmoothing(window_size=5)
slider = SlidingWindowInference(model, window_duration=1.0, overlap=0.5)

# Process live stream
stream_data = get_leap_motion_stream()  # Your data source
smoothed = smoother.smooth(stream_data)
predictions = slider(smoothed)
```

### Training on Google Colab

See [COLAB_TRAINING_GUIDE.md](COLAB_TRAINING_GUIDE.md) for a complete step-by-step guide with ready-to-run notebook cells.

---

## ğŸ“ Project Structure

```
Whisper_modification/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml                 # Model & training hyperparameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # Resampling, windowing, padding
â”‚   â”‚   â”œâ”€â”€ normalization.py        # Spatial, scale, feature normalization
â”‚   â”‚   â”œâ”€â”€ augmentation.py         # Masking, jitter, noise injection
â”‚   â”‚   â””â”€â”€ dataset.py              # PyTorch Dataset & DataLoader
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ frontend.py             # Patch Embedding + ConvSPE + Dropout
â”‚   â”‚   â”œâ”€â”€ positional.py           # RPE + Sinusoidal PE
â”‚   â”‚   â”œâ”€â”€ encoder.py              # S-MHSA + T-MHSA Transformer blocks
â”‚   â”‚   â”œâ”€â”€ decoder.py              # CTC + Attention two-pass decoder
â”‚   â”‚   â””â”€â”€ whisper_sign.py         # Main model class
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ losses.py               # Hybrid CTC-Attention loss
â”‚   â”‚   â”œâ”€â”€ scheduler.py            # Cosine warmup scheduler
â”‚   â”‚   â””â”€â”€ trainer.py              # 3-stage training orchestrator
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ sliding_window.py       # Real-time sliding window inference
â”‚       â”œâ”€â”€ smoothing.py            # Moving average noise filter
â”‚       â””â”€â”€ mediapipe_extract.py    # Video â†’ hand keypoints extraction
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                    # CLI training entry point
â”‚   â”œâ”€â”€ smoke_test.py               # Quick sanity check
â”‚   â”œâ”€â”€ deep_test.py                # 68 structural tests
â”‚   â””â”€â”€ functional_test.py          # 44 functional tests
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_model.py               # Pytest unit tests
â”œâ”€â”€ COLAB_TRAINING_GUIDE.md         # Google Colab training guide
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸ”§ Configuration

All hyperparameters are managed through `configs/config.yaml`:

```yaml
model:
  frontend:
    num_joints: 42          # 21 left + 21 right hand joints
    num_features: 7         # x, y, z, vx, vy, vz, confidence
    patch_size: 4           # Temporal grouping factor
    d_model: 512            # Hidden dimension
  encoder:
    num_heads: 8            # Multi-head attention heads
    num_layers: 6           # Transformer blocks
    d_ff: 2048              # Feed-forward dimension
  decoder:
    vocab_size: 1296        # Number of sign glosses
    blank_id: 0             # CTC blank token

training:
  stage1: { epochs: 30,  lr: 1e-3, freeze_encoder: true }
  stage2: { epochs: 100, lr: 5e-5, alpha: 0.3 }
  stage3: { epochs: 30,  lr: 1e-5, alpha: 0.3 }
```

---

## ğŸ“š Data Format

### Input Data Structure

```
data/processed/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ features/          # .npy files, shape (T, 42, 7)
â”‚   â””â”€â”€ labels/            # .npy files, integer arrays
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ features/
â”‚   â””â”€â”€ labels/
â””â”€â”€ test/
    â”œâ”€â”€ features/
    â””â”€â”€ labels/
```

### Supported Data Sources

| Source | Joints | FPS | Notes |
|--------|--------|-----|-------|
| **Leap Motion** | 42 (2Ã—21) | 120 Hz | Highest accuracy, requires hardware |
| **MediaPipe** | 42 (2Ã—21) | 30-60 Hz | Camera-based, use `mediapipe_extract.py` |
| **Custom** | Any | Any | Resample to 60 Hz using `preprocessing.py` |

---

## ğŸ“ References

- [Whisper: Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) â€” Radford et al., 2022
- [CTC: Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) â€” Graves et al., 2006
- [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands.html) â€” Google, 2020

---

## ğŸ“„ License

This project is licensed under the MIT License.
